# 系统性深度学习模型训练与评估实验方案

## 1. 实验目标

使用当前框架训练并评估7种深度学习模型：ResNet50、InceptionV3、ViT-Base/16、Bilinear CNN、MAE + ViT、DINO + ViT、iBOT + ViT，完成蝴蝶兰叶片图像分类任务，并进行全面的性能对比分析。

## 2. 实验环境配置

* **硬件**：GPU加速（确保CUDA可用）

* **软件**：PyTorch Lightning、Hydra、timm、torchmetrics等

* **数据**：蝴蝶兰叶片图像数据集

## 3. 模型训练方案

### 3.1 超参数配置

为每种模型配置合适的超参数，包括：

* 学习率：根据模型大小和复杂度调整

* 批处理大小：根据GPU内存调整

* 优化器：AdamW

* 权重衰减：防止过拟合

* 迭代次数：最大50轮

* 早停机制：耐心值10，监控验证集损失

### 3.2 训练流程

1. **数据准备**：使用`MyDatasetDataModule`加载和预处理数据
2. **模型初始化**：使用`OrchidModel`类初始化不同架构的模型
3. **训练执行**：使用PyTorch Lightning的`Trainer`进行训练
4. **早停机制**：配置`EarlyStopping`回调
5. **模型保存**：使用`ModelCheckpoint`保存最佳模型

### 3.3 可视化集成

* **t-SNE可视化**：在训练过程中定期（每5轮）生成特征空间分布

* **Grad-CAM可视化**：在训练完成后生成模型关注区域热力图

## 4. 模型评估方案

### 4.1 评估指标

* 准确率(Accuracy)

* 精确率(Precision)

* 召回率(Recall)

* F1分数

* 混淆矩阵

* 计算效率(每秒处理图像数)

### 4.2 评估流程

1. **加载最佳模型**：从检查点加载每个模型的最佳版本
2. **测试集评估**：使用`test_step`进行全面评估
3. **性能记录**：记录所有评估指标
4. **结果汇总**：生成对比表格

## 5. 实验执行计划

### 5.1 模型训练顺序

1. ResNet50
2. InceptionV3
3. ViT-Base/16
4. Bilinear CNN
5. MAE + ViT
6. DINO + ViT
7. iBOT + ViT

### 5.2 配置文件管理

为每个模型创建或更新配置文件，确保超参数合理配置。

### 5.3 日志管理

* 使用TensorBoard记录训练过程

* 保存训练/验证损失曲线

* 记录训练时间和资源使用情况

## 6. 实验结果分析

### 6.1 性能对比

* 生成各模型在各项指标上的对比表格

* 绘制性能对比图表

### 6.2 可视化分析

* 分析t-SNE特征分布，比较不同模型的特征学习能力

* 分析Grad-CAM热力图，理解模型决策依据

### 6.3 模型优缺点总结

* 比较不同模型的准确性、效率和泛化能力

* 分析模型性能差异的原因

## 7. 实验报告生成

### 7.1 报告内容

* 实验背景和目标

* 实验环境和配置

* 训练过程和结果

* 模型性能对比分析

* 可视化结果解释

* 经验教训和改进方向

### 7.2 可复现性保障

* 保存所有配置文件

* 记录完整的实验日志

* 保存原始数据和预处理脚本

## 8. 代码实现计划

### 8.1 配置文件更新

* 为每个模型优化超参数配置

* 配置早停和模型检查点

* 配置GPU加速

### 8.2 训练脚本优化

* 集成t-SNE可视化到训练过程

* 添加训练时间记录

* 优化日志记录

### 8.3 评估脚本开发

* 开发统一的评估脚本

* 实现多模型对比功能

* 生成标准化的评估报告

### 8.4 可视化增强

* 优化t-SNE可视化，支持不同训练阶段对比

* 增强Grad-CAM可视化，支持多模型对比

## 9. 预期输出

### 9.1 模型文件

* 7个最佳模型检查点

* 完整的训练日志

### 9.2 评估结果

* 模型性能对比表格

* 混淆矩阵

* 计算效率对比

### 9.3 可视化结果

* t-SNE特征分布图像

* Grad-CAM热力图

* 训练/验证损失曲线

### 9.4 实验报告

* 详细的实验分析报告

* 模型性能对比分析

* 改进建议

## 10. 时间规划

1. **准备阶段**：1天（配置环境、检查数据）
2. **模型训练**：7-10天（每个模型1-2天）
3. **模型评估**：2天（统一评估所有模型）
4. **可视化生成**：2天（生成t-SNE和Grad-CAM图像）
5. **结果分析**：2天（分析性能差异和可视化结果）
6. **报告撰写**：2天（撰写完整实验报告）

## 11. 风险控制

* **GPU内存不足**：调整批处理大小

* **训练时间过长**：优化超参数，使用早停机制

* **模型过拟合**：增加数据增强，调整权重衰减

* **可视化生成失败**：确保特征提取函数正常工作

## 12. 改进方向

* 尝试更多数据增强策略

* 探索模型融合技术

* 尝试知识蒸馏

* 优化模型架构

* 探索自监督预训练

通过以上实验方案，我们将能够全面评估7种深度学习模型在蝴蝶兰叶片图像分类任务上的性能，为后续研究和应用提供有价值的参考。
